python ..\models\research\object_detection\model_main_tf2.py --model_dir=Tensorflow\workspace\models\plate\my_ssd_mobnet --pipeline_config_path=Tensorflow\workspace\models\plate\my_ssd_mobnet\pipeline.config --num_train_steps=10000

python ..\models\research\object_detection\model_main_tf2.py --model_dir=Tensorflow\workspace\models\plateimage\my_ssd_mobnet --pipeline_config_path=Tensorflow\workspace\models\plateimage\my_ssd_mobnet\pipeline.config --num_train_steps=100000

check point를 saved model로 변환
Tf2
1.)
python ..\models\research\object_detection\exporter_main_v2.py --input_type image_tensor --pipeline_config_path Tensorflow\workspace\models\plateimage\my_ssd_mobnet\pipeline.config --trained_checkpoint_dir Tensorflow\workspace\models\plateimage\my_ssd_mobnet --output_directory .\exported-models\my_model

2.) onnx용으로는 input_type이 float_image_tensor로 되어야 함.
python ..\models\research\object_detection\exporter_main_v2.py --input_type float_image_tensor --pipeline_config_path Tensorflow\workspace\models\plateimage\my_ssd_mobnet\pipeline.config --trained_checkpoint_dir Tensorflow\workspace\models\plateimage\my_ssd_mobnet --output_directory .\exported-models\my_model

Tf1
python ..\models\research\object_detection\export_inference_graph.py --input_type image_tensor --pipeline_config_path Tensorflow\workspace\models\car-plate\my_ssd_mobnet\pipeline.config --trained_checkpoint_prefix Tensorflow\workspace\models\car-plate\my_ssd_mobnet\ckpt-11 --output_directory inference_graph



saved_model_cli.exe show --dir c:\SPB_Data\RealTimeObjectDetection-main\exported-models\my_model\saved_model --tag_set serve --signature_def serving_default
The given SavedModel SignatureDef contains the following input(s):
  inputs['input_tensor'] tensor_info:
      dtype: DT_FLOAT
      shape: (-1, -1, -1, 3)
      name: serving_default_input_tensor:0
The given SavedModel SignatureDef contains the following output(s):
  outputs['detection_anchor_indices'] tensor_info:
      dtype: DT_FLOAT
      shape: (-1, 10)
      name: StatefulPartitionedCall:0
  outputs['detection_boxes'] tensor_info:
      dtype: DT_FLOAT
      shape: (-1, 10, 4)
      name: StatefulPartitionedCall:1
  outputs['detection_classes'] tensor_info:
      dtype: DT_FLOAT
      shape: (-1, 10)
      name: StatefulPartitionedCall:2
  outputs['detection_multiclass_scores'] tensor_info:
      dtype: DT_FLOAT
      shape: (-1, 10, 11)
      name: StatefulPartitionedCall:3
  outputs['detection_scores'] tensor_info:
      dtype: DT_FLOAT
      shape: (-1, 10)
      name: StatefulPartitionedCall:4
  outputs['num_detections'] tensor_info:
      dtype: DT_FLOAT
      shape: (-1)
      name: StatefulPartitionedCall:5
  outputs['raw_detection_boxes'] tensor_info:
      dtype: DT_FLOAT
      shape: (-1, -1, 4)
      name: StatefulPartitionedCall:6
  outputs['raw_detection_scores'] tensor_info:
      dtype: DT_FLOAT
      shape: (-1, -1, 11)
      name: StatefulPartitionedCall:7
Method name is: tensorflow/serving/predict

(object_detection_api) C:\venvs\object_detection_api\Scripts>



(object_detection_api) C:\venvs\object_detection_api\Scripts>saved_model_cli.exe show --dir c:\SPB_Data\RealTimeObjectDetection-main\exported-models\my_model\saved_model --tag_set serve --signature_def serving_default
The given SavedModel SignatureDef contains the following input(s):
  inputs['input_tensor'] tensor_info:
      dtype: DT_UINT8
      shape: (1, -1, -1, 3)
      name: serving_default_input_tensor:0
The given SavedModel SignatureDef contains the following output(s):
  outputs['detection_anchor_indices'] tensor_info:
      dtype: DT_FLOAT
      shape: (1, 10)
      name: StatefulPartitionedCall:0
  outputs['detection_boxes'] tensor_info:
      dtype: DT_FLOAT
      shape: (1, 10, 4)
      name: StatefulPartitionedCall:1
  outputs['detection_classes'] tensor_info:
      dtype: DT_FLOAT
      shape: (1, 10)
      name: StatefulPartitionedCall:2
  outputs['detection_multiclass_scores'] tensor_info:
      dtype: DT_FLOAT
      shape: (1, 10, 11)
      name: StatefulPartitionedCall:3
  outputs['detection_scores'] tensor_info:
      dtype: DT_FLOAT
      shape: (1, 10)
      name: StatefulPartitionedCall:4
  outputs['num_detections'] tensor_info:
      dtype: DT_FLOAT
      shape: (1)
      name: StatefulPartitionedCall:5
  outputs['raw_detection_boxes'] tensor_info:
      dtype: DT_FLOAT
      shape: (1, 85360, 4)
      name: StatefulPartitionedCall:6
  outputs['raw_detection_scores'] tensor_info:
      dtype: DT_FLOAT
      shape: (1, 85360, 11)
      name: StatefulPartitionedCall:7
Method name is: tensorflow/serving/predict

(object_detection_api) C:\venvs\object_detection_api\Scripts>


tensorflow --opset 11까지만 지원한다는 얘기가 있음.
python -m tf2onnx.convert --saved-model .\exported-models\my_model\saved-model --opset 13 --output model.onnx

python -m tf2onnx.convert --saved-model C:\SPB_Data\RealTimeObjectDetection-main\exported-models\my_model\saved_model --opset 11 --fold_const --output model.onnx


#debug command
debugfile("../models/research/object_detection/model_main_tf2.py", "--model_dir=Tensorflow/workspace/models/plateimage/my_ssd_mobnet --pipeline_config_path=Tensorflow/workspace/models/plateimage/my_ssd_mobnet/pipeline.config --num_train_steps=10000")



tensorboard --logdir ./Tensorflow/workspace/models/plateimage/my_ssd_mobnet/train


trtexec --onnx=model.onnx --saveEngine=engine.trt --verbose

trtexec --explicitBatch --onnx=model.onnx --saveEngine=model.engine


#object detection model 사용시
onnx 파일 만들기...
python .\tensorflow_object_detection_api\create_onnx.py --pipeline_config .\Tensorflow\workspace\models\plateimage\my_ssd_mobnet\pipeline.config --saved_model .\exported-models\my_model\saved_model --onnx .\exported-models\my_model\model.onnx
tensorrt 파일 만들기
python .\tensorflow_object_detection_api\build_engine.py --onnx .\exported-models\my_model\model.onnx --engine .\exported-models\my_model\engine.trt --precision fp16
inference 테스트

python .\tensorflow_object_detection_api\infer.py --engine .\exported-models\my_model\engine.trt --input .\Tensorflow\workspace\images\plateimage\test --output .\Tensorflow\workspace\images\plateimage\result --preprocessor fixed_shape_resizer --labels .\tensorflow_object_detection_api\plateimage_label.txt



# car-plate tensorflow model 만들기
python ..\models\research\object_detection\exporter_main_v2.py --input_type float_image_tensor --pipeline_config_path Tensorflow\workspace\models\car-plate\my_ssd_mobnet\pipeline.config --trained_checkpoint_dir Tensorflow\workspace\models\car-plate\my_ssd_mobnet --output_directory .\exported-models\car-plate
#onnx 만들기
python .\tensorflow_object_detection_api\create_onnx.py --pipeline_config .\Tensorflow\workspace\models\car-plate\my_ssd_mobnet\pipeline.config --saved_model .\exported-models\car-plate\saved_model --onnx .\exported-models\car-plate\model.onnx
#tensorrt 만들기
python .\tensorflow_object_detection_api\build_engine.py --onnx .\exported-models\car-plate\model.onnx --engine .\exported-models\car-plate\engine.trt --precision fp16


#plateimage tensorflow model 만들기
python ..\models\research\object_detection\exporter_main_v2.py --input_type float_image_tensor --pipeline_config_path Tensorflow\workspace\models\plateimage\my_ssd_mobnet\pipeline.config --trained_checkpoint_dir Tensorflow\workspace\models\plateimage\my_ssd_mobnet --output_directory .\exported-models\plateimage
#onnx 만들기
python .\tensorflow_object_detection_api\create_onnx.py --pipeline_config .\Tensorflow\workspace\models\plateimage\my_ssd_mobnet\pipeline.config --saved_model .\exported-models\plateimage\saved_model --onnx .\exported-models\plateimage\model.onnx
#tensorrt 만들기
python .\tensorflow_object_detection_api\build_engine.py --onnx .\exported-models\plateimage\model.onnx --engine .\exported-models\plateimage\engine.trt --precision fp16



(object_detection_api) C:\SPB_Data\RealTimeObjectDetection-main>python .\tensorflow_object_detection_api\build_engine.py --onnx .\exported-models\car-plate\model.onnx --engine .\exported-models\car-plate\engine.trt --precision fp16
[12/27/2022-19:42:04] [TRT] [I] [MemUsageChange] Init CUDA: CPU +396, GPU +0, now: CPU 23868, GPU 1441 (MiB)
[12/27/2022-19:42:05] [TRT] [I] [MemUsageSnapshot] Begin constructing builder kernel library: CPU 24073 MiB, GPU 1441 MiB
[12/27/2022-19:42:05] [TRT] [I] [MemUsageSnapshot] End constructing builder kernel library: CPU 24473 MiB, GPU 1563 MiB
.\tensorflow_object_detection_api\build_engine.py:129: DeprecationWarning: Use set_memory_pool_limit instead.
  self.config.max_workspace_size = workspace * (2 ** 30)
[12/27/2022-19:42:05] [TRT] [W] onnx2trt_utils.cpp:365: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.
[12/27/2022-19:42:05] [TRT] [I] No importer registered for op: EfficientNMS_TRT. Attempting to import as plugin.
[12/27/2022-19:42:05] [TRT] [I] Searching for plugin: EfficientNMS_TRT, plugin_version: 1, plugin_namespace:
[12/27/2022-19:42:05] [TRT] [I] Successfully created plugin: EfficientNMS_TRT
INFO:EngineBuilder:Network Description
INFO:EngineBuilder:Input 'input_tensor' with shape (1, 640, 640, 3) and dtype DataType.FLOAT
INFO:EngineBuilder:Output 'num_detections' with shape (1, 1) and dtype DataType.INT32
INFO:EngineBuilder:Output 'detection_boxes' with shape (1, 10, 4) and dtype DataType.FLOAT
INFO:EngineBuilder:Output 'detection_scores' with shape (1, 10) and dtype DataType.FLOAT
INFO:EngineBuilder:Output 'detection_classes' with shape (1, 10) and dtype DataType.INT32
INFO:EngineBuilder:Building fp16 Engine in C:\SPB_Data\RealTimeObjectDetection-main\exported-models\car-plate\engine.trt
.\tensorflow_object_detection_api\build_engine.py:222: DeprecationWarning: Use build_serialized_network instead.
  with self.builder.build_engine(self.network, self.config) as engine, open(engine_path, "wb") as f:
[12/27/2022-19:42:05] [TRT] [W] TensorRT was linked against cuBLAS/cuBLAS LT 11.8.0 but loaded cuBLAS/cuBLAS LT 11.4.1
[12/27/2022-19:42:05] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +724, GPU +272, now: CPU 25041, GPU 1835 (MiB)
[12/27/2022-19:42:06] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +253, GPU +264, now: CPU 25294, GPU 2099 (MiB)
[12/27/2022-19:42:06] [TRT] [W] TensorRT was linked against cuDNN 8.3.2 but loaded cuDNN 8.1.1
[12/27/2022-19:42:06] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.
[12/27/2022-19:42:34] [TRT] [I] Some tactics do not have sufficient workspace memory to run. Increasing workspace size will enable more tactics, please check verbose output for requested sizes.
[12/27/2022-19:43:30] [TRT] [I] Detected 1 inputs and 4 output network tensors.
[12/27/2022-19:43:30] [TRT] [I] Total Host Persistent Memory: 186144
[12/27/2022-19:43:30] [TRT] [I] Total Device Persistent Memory: 962048
[12/27/2022-19:43:30] [TRT] [I] Total Scratch Memory: 122761728
[12/27/2022-19:43:30] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 10 MiB, GPU 815 MiB
[12/27/2022-19:43:30] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 37.7609ms to assign 12 blocks to 168 nodes requiring 151891456 bytes.
[12/27/2022-19:43:30] [TRT] [I] Total Activation Memory: 151891456
[12/27/2022-19:43:30] [TRT] [W] TensorRT was linked against cuBLAS/cuBLAS LT 11.8.0 but loaded cuBLAS/cuBLAS LT 11.4.1
[12/27/2022-19:43:30] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 26115, GPU 2427 (MiB)
[12/27/2022-19:43:30] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 26115, GPU 2435 (MiB)
[12/27/2022-19:43:30] [TRT] [W] TensorRT was linked against cuDNN 8.3.2 but loaded cuDNN 8.1.1
[12/27/2022-19:43:30] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +6, GPU +12, now: CPU 6, GPU 12 (MiB)
INFO:EngineBuilder:Serializing engine to file: C:\SPB_Data\RealTimeObjectDetection-main\exported-models\car-plate\engine.trt


채널 순서 바꾸기

python .\tensorflow_object_detection_api\create_onnx.py --pipeline_config .\Tensorflow\workspace\models\plateimage\my_ssd_mobnet\pipeline.config --saved_model .\exported-models\plateimage\saved_model --onnx .\exported-models\plateimage\model.onnx -f NCHW




python create_onnx.py --input_shape '1,320,320,3' --saved_model .\exported-models\plateimage\saved_model --onnx .\exported-models\plateimage\model.onnx